Application
===========

The development layer abstracts the intricacies of generators, tokenization, and memory techniques. Instead of writing numerous lines of code for setting up and serving, users can initiate their application with just a few method calls.

Cache
******

MaxCache
^^^^^^^^^
MaxCache class for storing and retrieving data with optional support for semantic caching.

Args:
    - ``semantic (bool)``: Flag to indicate if semantic caching is enabled. If True, an embedding model must be provided, defaults to False.
    - ``ttl (Optional[int])``: The time-to-live (TTL) for cache entries in seconds. If None, entries are stored indefinitely, defaults to None.
    - ``cache_type (str)``: The type of cache to use. Currently, only 'redis' is supported, defaults to 'redis'.
    - ``embedding_model (Optional[Any])``: The embedding model to use for semantic caching. Required if semantic is True, defaults to None.

Attributes:
    - ``redis_url``: The URL for the Redis connection.
    - ``cache``: The cache instance, either RedisSemanticCache or RedisCache.

Raises:
    - ``ValueError``: If `cache_type` is not 'redis', indicating an unsupported cache type, or if `semantic` is True but no `embedding_model` is provided.


Methods:

    - ``lookup``: Retrieves the cached output for a given prompt and LLM string combination.

        - ``prompt (str)``: The original input prompt.
        - ``llm_string (str)``: The identifier for the LLM.

        - ``Returns``: The text output retrieved from the cache. If no matching entry is found in the cache, None is returned.
        - ``Return type``: Optional[str]

    - ``update``: Updates the cache with the output generated by the large language model (LLM) for a given prompt and LLM string.

        - ``prompt (str)``: The original input prompt.
        - ``llm_string (str)``: The identifier for the LLM.
        - ``output (str)``: The text output generated by the model.

.. code-block:: python

    from maxaillm.app.cache.cache import MaxCache

    # Initialize a MaxCache
    cache = MaxCache(
        semantic=True,
        embedding_model=emb.model
    )

    # Example of retrieving cached output 
    output = cache.lookup("What is the weather like today?", "GPT-4")

    # Example of updating cache
    collection = "collection_name"
    query = "question asked by user"
    resp = "response by LLM"
            cache.update(query, collection, resp)    # save the results for a particular collection


Generator
************

MaxGenerator
^^^^^^^^^^^^
A generator is used for generating responses from LLM.
It is capable of generating both single responses and a stream of responses based on queries, context, and conversational history.

Args:
    - ``llm (LLM)``: The large language model instance.
    - ``method (str)``: The method to be used for generating responses or processing text.
    - ``prompt_config (dict)``: Configuration settings for prompts.
    - ``streamable (bool, Optional)``: Flag to indicate if the data should be processed as a stream. Defaults to False.
    - ``context_window (int, Optional)``: The size of the context window in terms of the number of tokens. Defaults to 10000.
    - ``verbose (bool, Optional)``: Flag to indicate if verbose mode is enabled. Defaults to True.
    - ``chat_history (bool, Optional)``: Flag to indicate if the conversation history should be maintained. Defaults to False.

Attributes:
    - ``streamable``: A flag indicating whether the generator is capable of streaming responses.
    - ``llm``: An instance of a language model (LLM) used for generating responses.
    - ``method``: The method used for generating responses.
    - ``context_window``: The size of the context window for generating responses.
    - ``prompt_config``: Configuration settings for prompts.
    - ``verbose``: Flag to indicate if verbose mode is enabled.
    - ``chat_history``: Flag to indicate if the conversation history should be maintained.

Methods:
    - ``_create_template``: Creates a chat prompt template based on the configuration and chat history.

        - ``None``: No arguments.

    - ``_initialize_chain``: Initializes the question-answering chain with a specific prompt template and method.

        - ``None``: No arguments.

    - ``generate``: Generates a response based on queries, context, and conversational history.

        - ``query (str)``: The user's query.
        - ``context (List[str])``: List of contexts associated with each query.
        - ``conversation (List[dict], Optional)``: Conversational context. Defaults to an empty list.

    - ``generate_async``: Asynchronously generates a response based on queries, context, and conversational history.

        - ``query (str)``: The user's query.
        - ``context (List[str])``: List of contexts associated with each query.
        - ``conversation (List[dict], Optional)``: Conversational context. Defaults to an empty list.

    - ``generate_stream``: Generates a stream of responses based on queries, context, and conversational history.

        - ``query (str)``: The user's query.
        - ``context (List[str])``: List of contexts associated with each query.
        - ``conversation (List[dict], Optional)``: Conversational context. Defaults to an empty list.

    - ``prepare_messages``: Prepares messages formatted for a chatbot system using GPT-4 model.

        - ``context (List[str])``: Context information for the queries.
        - ``conversation (List[dict], Optional)``: Previous conversation messages with role and content. Defaults to an empty list.

    - ``set_generator``: Sets the currently active generator.

        - ``generator (str)``: The name of the generator to be set as active.

    - ``get_generators``: Returns the available generators.

        - ``None``: No arguments.

    - ``calculate_tokens``: Calculates the number of tokens in the formatted response.

        - ``prompt_config (dict)``: The configuration settings for the prompt.
        - ``context (List[str])``: The context for the query.
        - ``query (str)``: The query to generate a response for.
        - ``chat (str)``: The chat history.

.. code-block:: python

    from maxaillm.app.generator.MaxGenerator import MaxGenerator
    from maxaillm.model.llm import MaxOpenAILLM

    # Define prompt configuration
    prompt_config = {"output_format": "The weather is gloomy", "max_tokens": 150}

    # Define LLM
    llm = MaxOpenAILLM("gpt-3")

    # Initialize MaxGenerator
    generator = MaxGenerator(
        llm=llm,
        method="greedy",
        prompt_config=prompt_config,
        engine="langchain",
        streamable=True,
        verbose=True,
        chat_history=True,
    )

    # Example of generating batch response
    response = generator.generate(
        query="What is the weather like today?",
        context=["Sunny in San Francisco."],
        conversation=[{"human": "How is the weather?", "assistant": "Checking..."}],
    )

    # Example of generating a stream of responses
    async for token in generator.generate_stream(
        query="Tell me about the history of AI",
        context=["AI has a rich history starting from the 1950s."],
        conversation=[{"human": "Tell me about AI", "assistant": "Sure, let me look it up."}],
    ):
        print(token, end="")

    # Example of preparing messages
    context, buffer = generator.prepare_messages(
        context=["Sunny in San Francisco."], conversation=[{"human": "How is the weather?", "assistant": "Checking..."}]
    )
    

LangChainGenerator
^^^^^^^^^^^^^^^^^^^^^^^^
A LangChainGenerator used for generating responses from LLM. It is capable of generating both single responses and a stream of responses based on queries, context, and conversational history.

Args:
    - ``llm (LLM)``: The large language model instance.
    - ``method (str)``: The method to be used for generating responses or processing text.
    - ``prompt_config (dict)``: Configuration settings for prompts.
    - ``streamable (bool, Optional)``: Flag to indicate if the data should be processed as a stream. Defaults to False.
    - ``context_window (int, Optional)``: The size of the context window in terms of the number of tokens. Defaults to 10000.
    - ``verbose (bool, Optional)``: Flag to indicate if verbose mode is enabled. Defaults to True.
    - ``chat_history (bool, Optional)``: Flag to indicate if the conversation history should be maintained. Defaults to False.

Attributes:
    - ``streamable``: A flag indicating whether the generator is capable of streaming responses.
    - ``llm``: An instance of a language model (LLM) used for generating responses.
    - ``method``: The method used for generating responses.
    - ``context_window``: The size of the context window for generating responses.
    - ``prompt_config``: Configuration settings for prompts.
    - ``verbose``: Flag to indicate if verbose mode is enabled.
    - ``chat_history``: Flag to indicate if the conversation history should be maintained.

Methods:
    - ``_create_template``: Creates a chat prompt template based on the configuration and chat history.

        - ``None``: No arguments.

    - ``_initialize_chain``: Initializes the question-answering chain with a specific prompt template and method.

        - ``None``: No arguments.

    - ``generate``: Generates a response based on queries, context, and conversational history.

        - ``query (str)``: The user's query.
        - ``context (List[str])``: List of contexts associated with each query.
        - ``conversation (List[dict], Optional)``: Conversational context. Defaults to an empty list.

    - ``generate_async``: Asynchronously generates a response based on queries, context, and conversational history.

        - ``query (str)``: The user's query.
        - ``context (List[str])``: List of contexts associated with each query.
        - ``conversation (List[dict], Optional)``: Conversational context. Defaults to an empty list.

    - ``generate_stream``: Generates a stream of responses based on queries, context, and conversational history.

        - ``query (str)``: The user's query.
        - ``context (List[str])``: List of contexts associated with each query.
        - ``conversation (List[dict], Optional)``: Conversational context. Defaults to an empty list.

    - ``prepare_messages``: Prepares messages formatted for a chatbot system using GPT-4 model.

        - ``context (List[str])``: Context information for the queries.
        - ``conversation (List[dict], Optional)``: Previous conversation messages with role and content. Defaults to an empty list.

.. code-block:: python

    from maxaillm.app.generator.MaxGenerator import LangChainGenerator
    from maxaillm.model.llm import MaxOpenAILLM

    # Define prompt configuration
    p_conf = {"moderations": "", "task": "", "identity": ""}

    # Define LLM
    llm = MaxOpenAILLM("gpt-3")

    # Initialize LangChainGenerator
    generator = LangChainGenerator(
        llm=llm, method="text-generation", prompt_config=p_conf, streamable=True, context_window=2048
    )

    # Example of generating batch response
    response = generator.generate(
        query="Explain Reinforcement Learning", context=["Reinforcement learning is a type of machine learning."]
    )

    # Example of generating stream response
    async for token in generator.generate_stream(
        query="Explain Reinforcement Learning", context=["Reinforcement learning is a type of machine learning."]
    ):
        print(token, end="")

        
Memory
******

MaxMemory
^^^^^^^^^
MaxMessage represents a conversation message, including attributes for the human input, assistant response, feedback, sources, and comments.

Args:
    - ``id (str, Optional)``: A unique identifier for the conversation or interaction instance.
    - ``human (str, Optional)``: The text input by the human participant in the conversation.
    - ``assistant (str, Optional)``: The text generated by the assistant in response to the human input.
    - ``feedback (bool, Optional)``: A boolean flag indicating whether feedback was provided.
    - ``sources (Dict, Optional)``: A dictionary containing sources or references that were used.
    - ``comments (str, Optional)``: Additional comments or notes about the conversation instance.

Attributes:
    - ``id``: A unique identifier for the conversation or interaction instance.
    - ``human``: The text input by the human participant in the conversation.
    - ``assistant``: The text generated by the assistant in response to the human input.
    - ``feedback``: A boolean flag indicating whether feedback was provided.
    - ``sources``: A dictionary containing sources or references that were used.
    - ``comments``: Additional comments or notes about the conversation instance.

Methods:
    - ``__str__``: Converts the instance attributes into a JSON string representation.
    - ``get_lc_namespace``: Get the namespace of the LangChain object.

.. code-block:: python

    from maxaillm.app.memory.memory import MaxMessage

    # Initialize a MaxMessage
    conversation = MaxMessage(
        id="12345",
        human="How does photosynthesis work?",
        assistant="Photosynthesis is the process by which green plants use sunlight to synthesize foods...",
        feedback=True,
        sources={"Wikipedia": "https://en.wikipedia.org/wiki/Photosynthesis"},
        comments="This was a test conversation."
    )
    
CustomMessage
^^^^^^^^^^^^^
CustomMessage - SQLAlchemy model for storing conversation messages in a database.

Attributes:
    - ``id``: The unique identifier of the message.
    - ``session_id``: The session identifier for the message.
    - ``human``: The human input text.
    - ``assistant``: The assistant's response text.
    - ``feedback``: Feedback provided for the conversation.
    - ``comments``: Additional comments about the conversation.
    - ``sources``: Sources or references used in the conversation.
    - ``created_at``: The timestamp when the message was created.

Methods:
    - ``as_dict``: Converts the SQLAlchemy model instance to a dictionary.

.. code-block:: python

    from maxaillm.app.memory.memory import CustomMessage
    from datetime import datetime

    # Example of creating a new CustomMessage instance
    new_message = CustomMessage(
        id="123",
        session_id="12345",
        human="Hello, how are you?",
        assistant="I am good, thank you!",
        feedback=True,
        comments="Helpful response",
        sources={"source1": "example.com"},
        created_at=datetime.utcnow()
    )

    # Example of converting the message instance to a dictionary
    message_dict = new_message.as_dict()
    

CustomMessageConverter
^^^^^^^^^^^^^^^^^^^^^^
CustomMessageConverter converts between MaxMessage and CustomMessage objects.

Methods:
    - ``from_sql_model``: Converts a SQL model instance to a dictionary, filtering out unwanted keys.
    
        - ``sql_message (Any)``: The SQL model instance to be converted.
    - ``to_sql_model``: Converts a MaxMessage object into a CustomMessage object.
    
        - ``message (MaxMessage)``: The MaxMessage object to be converted.
        - ``session_id (str)``: A unique identifier for the session.

    - ``get_sql_model_class``: Retrieves the SQL model class.


.. code-block:: python

    from datetime import datetime
    from maxiaillm.app.app.memory import CustomMessage, CustomMessageConverter, MaxMessage

    # Initialize the converter
    converter = CustomMessageConverter()

    # Example of converting a CustomMessage (SQL model) to a dictionary
    message = CustomMessage(
        id="123",
        session_id="12345",
        human="Hello, how are you?",
        assistant="I am good, thank you!",
        feedback=True,
        comments="Helpful response",
        sources={"source1": "example.com"},
        created_at=datetime.utcnow(),
    )
    filtered_dict = converter.from_sql_model(message)
    print(filtered_dict)

    # Example of converting a MaxMessage to a CustomMessage (SQL model)
    max_message = MaxMessage(
        id="123",
        human="How does photosynthesis work?",
        assistant="Photosynthesis is the process by which green plants use sunlight to synthesize foods...",
        feedback=True,
        sources={"Wikipedia": "https://en.wikipedia.org/wiki/Photosynthesis"},
        comments="This was a test conversation.",
    )
    session_id = "session_456"
    custom_message = converter.to_sql_model(max_message, session_id)
    print(custom_message)

    # Example of retrieving the SQL model class
    model_class = converter.get_sql_model_class()
    print(model_class)
    
    
Providers
*********

LLMProvider
^^^^^^^^^^^
Provides a method to get the appropriate LLM provider class and model name based on a given key.

Methods:
    - ``get_provider``: Returns the LLM provider class and model name based on the provided key.

        - ``key (str)``: The key representing the LLM provider.
        
Raises:
    - ``ValueError``: If the provided key is not supported.

Returns:
    - ``Type``: the instance of LLMProvider class.
        
.. code-block:: python

    from maxaillm.app.providers.providers import LLMProvider

    # Example of getting LLM provider
    llm_class, model_name = LLMProvider.get_provider("anthropic")
    print(llm_class)  # Outputs: <class 'maxaillm.model.llm.MaxAnthropicLLM'>
    print(model_name)  # Outputs: "claude-2"
    

VectorDBProvider
^^^^^^^^^^^^^^^^
Provides a method to get the appropriate Vector DB provider class based on a given key.

Methods:
    - ``get_provider``: Returns the Vector DB provider class based on the provided key.

        - ``key (str)``: The key representing the Vector DB provider.

Raises:
    - ``ValueError``: If the provided key is not supported.

Returns:
    - ``Type``: the instance of VectorDBProvider class.

.. code-block:: python

    from maxaillm.app.providers.providers import VectorDBProvider

    # Example of getting vector DB provider
    vectordb_class = VectorDBProvider.get_provider("pgvector")
    print(vectordb_class)  # Outputs: <class 'maxaillm.data.vectorstore.MaxPGVector'>
    

EmbeddingProvider
^^^^^^^^^^^^^^^^^
EmbeddingProvider provides a method to get the appropriate Embedding provider class and default arguments based on a given key.

Methods:
    - ``get_provider``: Returns the Embedding provider class and default arguments based on the provided key.

        - ``key (str)``: The key representing the Embedding provider.

Raises:
    - ``ValueError``: If the provided key is not supported.

Returns:
    - ``Tuple[Type, Dict]``: A tuple containing the Embedding provider class and a dictionary of default arguments.

.. code-block:: python

    from maxaillm.app.providers.providers import EmbeddingProvider

    # Example of getting embedding provider
    provider_class, args = EmbeddingProvider.get_provider('openai')
    print(provider_class)  # Outputs: <class 'maxaillm.data.embeddings.MaxOpenAIEmbeddings'>
    print(args)  # Outputs: {'dimensions': 256}
    
    
Templates
*********

MaxLLMJsonTemplate
^^^^^^^^^^^^^^^^^^
A class to handle the generation of Pydantic classes based on JSON formats and predictions from a language model.

Args:
    - ``llm (Any)``: The language model to be used.

Attributes:
    - ``llm``: The language model instance.
    - ``pydantic_output_parser``: The Pydantic output parser instance.

Methods:
    - ``__get_pydantic_class_from_code__``: Executes the provided Python code to get the Pydantic class.

        - ``python_code (str)``: A string containing the Python code to be executed.

        Returns:
        - ``Type``: The Pydantic class defined in the provided Python code.

    - ``get_pydantic_object``: Generates a Pydantic class based on the provided JSON format.

        - ``json_str (str)``: A JSON-formatted string to generate the Pydantic class.
        - ``save_pydantic_file (bool, Optional)``: If True, saves the generated Pydantic class code to a file. Default is False.
        - ``pydantic_file_save_path (str, Optional)``: File path to save the generated Pydantic class code. Defaults to 'pydantic_class.py' if not provided.
        - ``print_code (bool, Optional)``: If True, prints the generated Pydantic class code. Default is False.

        Returns:
        - ``Type``: The generated Pydantic class.

    - ``get_prompt_json``: Gets the Pydantic output parser based on the provided Pydantic object.

        - ``pydantic_object (PydanticBaseModel)``: The Pydantic object used to initialize the PydanticOutputParser.

        Returns:
        - ``PydanticOutputParser``: The initialized PydanticOutputParser.

    - ``retry_parser``: Retries parsing the response using the RetryWithErrorOutputParser.

        - ``bad_response (str)``: The response that failed to parse.
        - ``prompt (PromptTemplate)``: The prompt used for the initial prediction.

        Returns:
        - ``Any``: The parsed response after retrying.

    - ``autofixing_parser``: Attempts to automatically fix and parse a misformatted response.

        - ``misformatted (str)``: The misformatted response.

        Returns:
        - ``Any``: The fixed and parsed response.

    - ``predict``: Makes a prediction using the language model and parses the response using a Pydantic model.

        - ``pydantic_object (PydanticBaseModel)``: The Pydantic object used for parsing the response.
        - ``prompt_template (PromptTemplate)``: The template for the language model prompt.
        - ``prompt_kwargs (dict, Optional)``: Additional keyword arguments for formatting the prompt. Default is an empty dictionary.
        - ``json_placeholder (str, Optional)``: The placeholder in the prompt for inserting the Pydantic JSON instructions. Default is 'pydantic_json'.
        - ``return_type (str, Optional)``: The desired format for the parsed response. Can be 'json', 'dict', or 'pydantic'. Default is 'dict'.

        Returns:
        - ``Any``: The parsed response in the specified format.

.. code-block:: python

    from maxaillm.model.llm import MaxAnthropicLLM
    from maxaillm.app.template.json_template import MaxLLMJsonTemplate

    # Initialize the LLM
    llm = MaxAnthropicLLM("claude-2.1")

    # Initialize the JSON template formatter
    json_formatter = MaxLLMJsonTemplate(llm=llm)

    # Define JSON string for generating Pydantic class
    json_str = {
        "sentiment": "Get sentiment of the product MAX based on the given description",
        "summary": "Create a summary of the MAX Product",
    }

    # Example of generating Pydantic class from JSON string
    pydantic_class = json_formatter.get_pydantic_object(json_str, save_pydantic_file=True)

    # Example of getting Pydantic output parser
    parser = json_formatter.get_prompt_json(pydantic_class)

    # Example of automatically fixing and parsing a misformatted response
    misformatted_response = "{\\'key\\': \\'value\\'}"
    fixed_response = json_formatter.autofixing_parser(misformatted_response)
    print(fixed_response)